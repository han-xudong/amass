{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AMASS for training DNNs for Human Body and Motion\n",
    "AMASS is a large database of human bodies, ready to enable proper deep learning on human body. Instead of images, as in [ImageNet](http://www.image-net.org/),\n",
    " AMASS consists of human body parameters, controlling the surface mesh of the [SMPL](http://smpl.is.tue.mpg.de/) body model family.\n",
    "\n",
    "Here we provide basic tools to turn AMASS bodies into suitable formats for deep learning frameworks.\n",
    "We produce PyTorch readable *.pt*, as well as *.h5* files.\n",
    "\n",
    "The provided data preparation code has three stages that could be flexibly customized to your own specific needs.\n",
    "\n",
    "**Stage I** goes over the previously downloaded numpy npz files, sub-samples the time-length and dumps the result into one place as PyTorch pt files.\n",
    "\n",
    "**Stage II** uses PyTorch to apply all sorts of data augmentations in parallel on the original data and produces HDF5 files.\n",
    "HDF5 makes it possible to write files in chunks to avoid memory shortage,\n",
    "while PyTorch speeds up data augmentation though batch processing .\n",
    "\n",
    "**Stage III** simply turns the h5 files into pt files again to be readily usable by PyTorch.\n",
    "One can replace this stage's output to any other suitable kind.\n",
    "\n",
    "The progress at all stages is logged and could be inspected at any time during the process or later.\n",
    "We suggest using an experiment ID that would help in referring to a specific, traceable, data preparation run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies to run this notebook:\n",
    "- [Human Body Prior](https://github.com/nghorbani/human_body_prior)\n",
    "- [PyTorch>=1.7.0](https://pytorch.org/)\n",
    "- [PyTables](https://www.pytables.org/usersguide/installation.html)\n",
    "- [tqdm](https://pypi.org/project/tqdm/2.2.3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you continue with this tutorial, it is recommended to first take a look at the [**AMASS Visualization**](01-AMASS_Visualization.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path as osp\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "support_dir = '../support_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Choose the device to run the body model on.\n",
    "comp_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_body_prior.tools.omni_tools import log2file, makepath\n",
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first pick an experiment ID that later helps us to identify the specific run of data preparation and augmentations.\n",
    "We also prepare a message about this specific experiment, that is going to be included in our log file as future notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_code = 'VXX_SVXX_TXX' #VERSION_SUBVERSION_TRY\n",
    "\n",
    "msg = ''' Initial use of standard AMASS dataset preparation pipeline '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you have to download the body npz files from [AMASS](https://amass.is.tue.mpg.de/).\n",
    "You would need to first register and agree to the license and pick the subsets you like from datasets.\n",
    "Uncompress the body data at your desired location. The final folder structure should be like the following:\n",
    "\n",
    "**amass_dir>sub_dataset>subjects>*_poses.npz**\n",
    "\n",
    "Now specify the directory of the downloaded npz files and the final output folder for the prepared data, e.g. **work_dir**.\n",
    "At this location at each stage of the data processing pipeline, an appropriate folder is created.\n",
    "Moreover, the log file, and a snapshot of the code is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amass_dir =  '../support_data/amass_npz' #'PATH_TO_DOWNLOADED_NPZFILES/*/*_poses.npz'\n",
    "\n",
    "work_dir = '../support_data/prepared_data/VXX_SVXX_TXX'\n",
    "\n",
    "logger = log2file(makepath(work_dir, '%s.log' % (expr_code), isfile=True))\n",
    "logger('[%s] AMASS Data Preparation Began.'%expr_code)\n",
    "logger(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to specify data splits for train/validation/test. Below is the recommended data splits by AMASS\n",
    "for train/validation/test that chooses data for each split from non-overlapping datasets. \n",
    "One reason is to avoid similarity of marker layouts used for the original data.\n",
    "Remember, AMASS is growing, and it is a good idea to check the website for new datasets.\n",
    "You can also follow [AMASS twitter](https://twitter.com/mocap_amass) account for news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amass_splits = {\n",
    "#     'vald': ['HumanEva', 'MPI_HDM05', 'SFU', 'MPI_mosh'],\n",
    "#     'test': ['Transitions_mocap', 'SSM_synced'],\n",
    "#     'train': ['CMU', 'MPI_Limits', 'TotalCapture', 'Eyes_Japan_Dataset', 'KIT',\n",
    "#               'BML', 'EKUT', 'TCD_handMocap', 'ACCAD']\n",
    "# }\n",
    "amass_splits = {\n",
    "    'vald': ['SFU',],\n",
    "    'test': ['SSM_synced'],\n",
    "    'train': ['MPI_Limits']\n",
    "}\n",
    "amass_splits['train'] = list(set(amass_splits['train']).difference(set(amass_splits['test'] + amass_splits['vald'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything in place to run the preparation code.\n",
    "Take a look at the code for **prepare_amass** and customize it to your need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amass.data.prepare_data import prepare_amass\n",
    "prepare_amass(amass_splits, amass_dir, work_dir, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the prepared data\n",
    "Now we can try to open them with PyTorch which a dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import glob \n",
    "\n",
    "class AMASS_DS(Dataset):\n",
    "    \"\"\"AMASS: a pytorch loader for unified human motion capture dataset. http://amass.is.tue.mpg.de/\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_dir, num_betas=16):\n",
    "\n",
    "        self.ds = {}\n",
    "        for data_fname in glob.glob(os.path.join(dataset_dir, '*.pt')):\n",
    "            k = os.path.basename(data_fname).replace('.pt','')\n",
    "            self.ds[k] = torch.load(data_fname)\n",
    "        self.num_betas = num_betas\n",
    "\n",
    "    def __len__(self):\n",
    "       return len(self.ds['trans'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data =  {k: self.ds[k][idx] for k in self.ds.keys()}\n",
    "        data['root_orient'] = data['pose'][:3]\n",
    "        data['pose_body'] = data['pose'][3:66]\n",
    "        data['pose_hand'] = data['pose'][66:]\n",
    "        data['betas'] = data['betas'][:self.num_betas]\n",
    "\n",
    "        return data\n",
    "\n",
    "num_betas = 16 # number of body parameters\n",
    "testsplit_dir = os.path.join(work_dir, 'stage_III', 'test')\n",
    "\n",
    "ds = AMASS_DS(dataset_dir=testsplit_dir, num_betas=num_betas)\n",
    "print('Test split has %d datapoints.'%len(ds))\n",
    "\n",
    "batch_size = 5\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the tutorial in [**AMASS Visualization**](01-AMASS_Visualization.ipynb) to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "from body_visualizer.tools.vis_tools import colors, imagearray2file\n",
    "from body_visualizer.mesh.mesh_viewer import MeshViewer\n",
    "from body_visualizer.tools.vis_tools import show_image\n",
    "\n",
    "imw, imh = 1600, 1600\n",
    "mv = MeshViewer(width=imw, height=imh, use_offscreen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "\n",
    "bm_fname = osp.join(support_dir, 'body_models/smplh/male/model.npz')\n",
    "\n",
    "num_betas = 16 # number of body parameters\n",
    "num_dmpls = 8 # number of DMPL parameters\n",
    "\n",
    "bm = BodyModel(bm_fname=bm_fname, num_betas=num_betas).to(comp_device)\n",
    "faces = c2c(bm.f)\n",
    "\n",
    "bdata = next(iter(dataloader))\n",
    "body_v = bm.forward(**{k:v.to(comp_device) for k,v in bdata.items() if k in ['pose_body', 'betas']}).v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angles = [0, 180, 90, -90]\n",
    "images = np.zeros([len(view_angles), batch_size, 1, imw, imh, 3])\n",
    "for cId in range(0, batch_size):\n",
    "\n",
    "    orig_body_mesh = trimesh.Trimesh(vertices=c2c(body_v[cId]), faces=c2c(bm.f), vertex_colors=np.tile(colors['grey'], (6890, 1)))\n",
    "\n",
    "    for rId, angle in enumerate(view_angles):\n",
    "        if angle != 0: orig_body_mesh.apply_transform(trimesh.transformations.rotation_matrix(np.radians(angle), (0, 1, 0)))\n",
    "        mv.set_meshes([orig_body_mesh], group_name='static')\n",
    "        images[rId, cId, 0] = mv.render()\n",
    "\n",
    "        if angle != 0: orig_body_mesh.apply_transform(trimesh.transformations.rotation_matrix(np.radians(-angle), (0, 1, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imagearray2file(images)\n",
    "show_image(np.array(img)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above image each column is a data point and at each row we rotate the body to visualize it from different angles.\n",
    "AMASS has been recently used to train a human body prior for advanced human body inverse kinematics.\n",
    "Take a look at the paper [Expressive Body Capture: 3D Hands, Face, and Body from a Single Image](https://smpl-x.is.tue.mpg.de/)\n",
    "and the [VPoser](https://github.com/nghorbani/human_body_prior)  GitHub page for further information.\n",
    "VPoser is capable of generating new body poses for SMPL body model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
